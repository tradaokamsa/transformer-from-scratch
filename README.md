
# Transformer 

This is my Pytorch implementation of a Transformer model and its associated training code. 

The Transformer model, introduced in the paper "Attention is All You Need" by Vaswani et al., is a powerful architecture that relies on self-attention mechanisms to handle sequential data efficiently.


![](https://i.sstatic.net/nQ2f5.png)


## Reference

 - [Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is All you Need. arXiv (Cornell University), 30, 5998â€“6008](https://arxiv.org/pdf/1706.03762v5)


